{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QEzZ6ksYeqCr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ZZ3tMgyAGcd"
   },
   "source": [
    "## Activation Functions\n",
    "Neural Network example:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1eaFYKdTDJhQsnY291Leo_rsnIDTvBG0m\" width=\"400\" height=\"200\" />\n",
    "\n",
    "\n",
    "$𝑧_1 = (𝑥_1 ∗ 𝑤_1) + (𝑥_2 ∗ 𝑤_2) + 𝑏_1$  \n",
    "$𝑧_1$ =(0.1∗0.15)+(0.2∗0.05)+0.33 \n",
    "$𝑧_1$ = 0.355\n",
    "\n",
    "For Signmoid activation function, output values:  \n",
    "$𝑎_1 =𝑓(𝑧_1)= {1\\over 1+ 𝑒^{−(0.355)}}$ = 0.588 \n",
    "\n",
    "$𝑧_2 = (𝑎_1 ∗ 𝑤_2) + 𝑏_2$ \n",
    "\n",
    "$𝑧_2 = (0.588 ∗ 0.36) + 0.56$   \n",
    "$𝑧_2$ = 0.772 \n",
    "\n",
    "$𝑎_2 =𝑓(𝑧_2)= {1\\over 1+ 𝑒^{−(0.772)}}$ = 0.648 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8FI1C0mbgnCu"
   },
   "outputs": [],
   "source": [
    "# Helper function to calculate the node value of hidden layer 1\n",
    "# Hidden layer1 consists of only one node\n",
    "def output_hidden_layer_1(a_1):\n",
    "    b_1 = tf.constant(0.33)\n",
    "    w_2 = tf.constant(0.36)\n",
    "    b_2 = tf.constant(0.56)\n",
    "    z_2 = a_1*w_2 + b_2\n",
    "    return z_2\n",
    "\n",
    "# Helper function for a simple ReLu function\n",
    "def relu(input):\n",
    "    if input > 0:\n",
    "        return input\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "7oKBIh4dJYUU",
    "outputId": "e0431959-d59b-4482-b734-40d096015c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid function:  a_1 =  0.588 a_2 =  0.648\n",
      "Linear function:  a_1 =  0.355 a_2 =  0.6878\n",
      "Tanh function:  a_1 =  0.34080228 a_2 =  0.59656686\n",
      "ReLU function:  a_1 =  0.355 a_2 =  0.6878\n"
     ]
    }
   ],
   "source": [
    "# Neural net calculation\n",
    "z_1 = tf.constant(0.355)\n",
    "\n",
    "# Linear activation function\n",
    "a_1_lin = z_1\n",
    "z_2_lin = output_hidden_layer_1(a_1_lin) # recalculate z_2 from a_1_lin output\n",
    "a_2_lin = z_2_lin\n",
    "\n",
    "# Tanh activation function: (𝑒^𝑥 − 𝑒^(−𝑥)) / (𝑒^𝑥 + 𝑒^(−𝑥))\n",
    "a_1_tanh = tf.tanh(z_1)\n",
    "z_2_tanh = output_hidden_layer_1(a_1_tanh)\n",
    "a_2_tanh = tf.tanh(z_2_lin)\n",
    "\n",
    "# ReLu\n",
    "# In this case, Relu output will be the same as linear, since the outputs are\n",
    "# not linear, but to stay in the spirit of things, selection will be used\n",
    "a_1_relu = relu(z_1)\n",
    "z_2_relu = output_hidden_layer_1(a_1_relu)\n",
    "a_2_relu = relu(z_2_relu)\n",
    "\n",
    "# Call the .numpy() method to post only the value of the tensor in eager mode\n",
    "print(\"Sigmoid function: \", \"a_1 = \", 0.588, \"a_2 = \", 0.648)\n",
    "print(\"Linear function: \", \"a_1 = \", a_1_lin.numpy(), \"a_2 = \", a_2_lin.numpy())\n",
    "print(\"Tanh function: \", \"a_1 = \", a_1_tanh.numpy(), \"a_2 = \", a_2_tanh.numpy())\n",
    "print(\"ReLU function: \", \"a_1 = \", a_1_relu.numpy(), \"a_2 = \", a_2_relu.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Aqe6UWt6S40"
   },
   "source": [
    "### Gradients of Activation Functions\n",
    "<img src=\"https://drive.google.com/uc?id=1PIn0Gk3Dru9VzA3dj72ND9gCgIbxyFCO\" width=\"400\" height=\"200\" />\n",
    "\n",
    "ReLu is designed to help with the vanishing gradient problem.\n",
    "These gradients are derived with the chain rule.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FrzPlXUeVwa-"
   },
   "outputs": [],
   "source": [
    "# Gradient helper functions\n",
    "# Takes in tensorflow object as input\n",
    "def sigmoid_gradient(x):\n",
    "    grad = tf.sigmoid(x) * (1 - tf.sigmoid(x));\n",
    "    return grad\n",
    "\n",
    "def tanh_gradient(x):\n",
    "    grad = 1 - tf.square((tf.tanh(x)))\n",
    "    return grad\n",
    "\n",
    "def relu_gradient(x):\n",
    "    output_list = []\n",
    "    for val in x.numpy():\n",
    "        if val > 0:\n",
    "            output_list.append(1)\n",
    "        else:\n",
    "            output_list.append(0)        \n",
    "    output_tensor = tf.constant(output_list)\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "BYpKCZGl6qkz",
    "outputId": "3bd8d0cf-3e79-48c3-d7df-dc5580ec6039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Gradient:\n",
      "At x = -4:  Sigma prime =  0.017662734\n",
      "At x = 0.5:  Sigma prime =  0.23500372\n",
      "At x = 4:  Sigma prime =  0.017662734\n",
      "Tanh Gradient:\n",
      "At x = -4:  Tanh prime =  0.0013411045\n",
      "At x = 0.5:  Tanh prime =  0.7864477\n",
      "At x = 4:  Tanh prime =  0.0013411045\n",
      "ReLu Gradient:\n",
      "At x = -4:  ReLu prime =  0\n",
      "At x = 0.5:  ReLu prime =  1\n",
      "At x = 4:  ReLu prime =  1\n"
     ]
    }
   ],
   "source": [
    "# Input list\n",
    "# Make sure to set dtype to float, since default dtype for lists can give you\n",
    "# Integer vs float errors (-4 is interpreted as integer type, while 0.5 float)\n",
    "inputs = tf.constant([-4,0.5,4], dtype = \"float32\")\n",
    "\n",
    "# Outputs\n",
    "sigmoid_grad_outputs = sigmoid_gradient(inputs)\n",
    "tanh_grad_outputs = tanh_gradient(inputs)\n",
    "relu_grad_outputs = relu_gradient(inputs)\n",
    "\n",
    "# Print gradients at each point\n",
    "print(\"Sigmoid Gradient:\")\n",
    "print(\"At x = -4: \" , \"Sigma prime = \", sigmoid_grad_outputs.numpy()[0])\n",
    "print(\"At x = 0.5: \" , \"Sigma prime = \", sigmoid_grad_outputs.numpy()[1])\n",
    "print(\"At x = 4: \" , \"Sigma prime = \", sigmoid_grad_outputs.numpy()[2])\n",
    "print(\"Tanh Gradient:\")\n",
    "print(\"At x = -4: \" , \"Tanh prime = \", tanh_grad_outputs.numpy()[0])\n",
    "print(\"At x = 0.5: \" , \"Tanh prime = \", tanh_grad_outputs.numpy()[1])\n",
    "print(\"At x = 4: \" , \"Tanh prime = \", tanh_grad_outputs.numpy()[2])\n",
    "print(\"ReLu Gradient:\")\n",
    "print(\"At x = -4: \" , \"ReLu prime = \", relu_grad_outputs.numpy()[0])\n",
    "print(\"At x = 0.5: \" , \"ReLu prime = \", relu_grad_outputs.numpy()[1])\n",
    "print(\"At x = 4: \" , \"ReLu prime = \", relu_grad_outputs.numpy()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_JzvfnprTfI"
   },
   "source": [
    "### Softmax equation:\n",
    "Usually used only in the output layer of classification models.\n",
    "$$S(y_i) = {e^{y(i)}\\over \\sum_j e^{y_j}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eTO_1t8Kff8Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.63756594 0.21222727 0.08628517 0.06392162], shape=(4,), dtype=float64)\n",
      "tf.Tensor([0.21910707 0.1793897  0.44122746 0.04002725 0.12024851], shape=(5,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# Two random input logits\n",
    "V1 = np.array([2.3, 1.2, 0.3, 0.0])\n",
    "V2 = np.array([1.9, 1.7, 2.6, 0.2, 1.3])\n",
    "\n",
    "# Calculate output from input logits\n",
    "V1_softmax = tf.nn.softmax(V1)\n",
    "V2_softmax = tf.nn.softmax(V2)\n",
    "\n",
    "print(V1_softmax)\n",
    "print(V2_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5iHLiEf4Ain"
   },
   "source": [
    "## Binary Cross-Entropy / Log Loss\n",
    "\n",
    "𝐶𝑜𝑠𝑡 𝐹𝑢𝑛𝑐𝑡𝑖𝑜𝑛 = −( (𝑡𝑎𝑟𝑔𝑒𝑡 ∗ log(𝑐𝑜𝑚𝑝𝑉𝑎𝑙𝑢𝑒) + (1 − 𝑡𝑎𝑟𝑔𝑒𝑡) ∗ log(1 − 𝑐𝑜𝑚𝑝𝑉𝑎𝑙𝑢𝑒)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "rvrZ0fXoiQgh",
    "outputId": "9f125899-b7c5-4a0d-a3eb-a70a66194f02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Target = 0: \n",
      "Computed Value:  0.95  Cost Function:  2.995732\n",
      "Computed Value:  0.8  Cost Function:  1.609438\n",
      "Computed Value:  0.6  Cost Function:  0.9162908\n",
      "Computed Value:  0.4  Cost Function:  0.5108256\n",
      "Computed Value:  0.1  Cost Function:  0.105360545\n",
      "When Target = 1: \n",
      "Computed Value:  0.95  Cost Function:  0.051293306\n",
      "Computed Value:  0.8  Cost Function:  0.22314353\n",
      "Computed Value:  0.6  Cost Function:  0.5108256\n",
      "Computed Value:  0.4  Cost Function:  0.9162907\n",
      "Computed Value:  0.1  Cost Function:  2.3025851\n"
     ]
    }
   ],
   "source": [
    "computed_value= tf.constant([0.95,0.8,0.6,0.4,0.1])\n",
    "\n",
    "# Case for binary target of 1 or 0\n",
    "target = 0;\n",
    "cost_function_target_0 = -1 *( target*tf.math.log(computed_value) + (1-target) * \n",
    "                         tf.math.log(1 - computed_value) )\n",
    "target = 1;\n",
    "cost_function_target_1 = -1 *( target*tf.math.log(computed_value) + (1-target) * \n",
    "                         tf.math.log(1 - computed_value) )\n",
    "\n",
    "# Output results to terminal\n",
    "print(\"When Target = 0: \")\n",
    "print(\"Computed Value: \", computed_value.numpy()[0], \" Cost Function: \", cost_function_target_0.numpy()[0])\n",
    "print(\"Computed Value: \", computed_value.numpy()[1], \" Cost Function: \", cost_function_target_0.numpy()[1])\n",
    "print(\"Computed Value: \", computed_value.numpy()[2], \" Cost Function: \", cost_function_target_0.numpy()[2])\n",
    "print(\"Computed Value: \", computed_value.numpy()[3], \" Cost Function: \", cost_function_target_0.numpy()[3])\n",
    "print(\"Computed Value: \", computed_value.numpy()[4], \" Cost Function: \", cost_function_target_0.numpy()[4])\n",
    "print(\"When Target = 1: \")\n",
    "print(\"Computed Value: \", computed_value.numpy()[0], \" Cost Function: \", cost_function_target_1.numpy()[0])\n",
    "print(\"Computed Value: \", computed_value.numpy()[1], \" Cost Function: \", cost_function_target_1.numpy()[1])\n",
    "print(\"Computed Value: \", computed_value.numpy()[2], \" Cost Function: \", cost_function_target_1.numpy()[2])\n",
    "print(\"Computed Value: \", computed_value.numpy()[3], \" Cost Function: \", cost_function_target_1.numpy()[3])\n",
    "print(\"Computed Value: \", computed_value.numpy()[4], \" Cost Function: \", cost_function_target_1.numpy()[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GrujqRpzh47P"
   },
   "source": [
    "## Argmax Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RL6szg5oolxN"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=18f3-KiMgUU_Mez3HWWWSGRbcT0SVeKpq\" width=\"600\" height=\"200\" />\n",
    "\n",
    "\n",
    "Image taken from: Physics Dept, Cornell University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Ua3kI5lTh6CV",
    "outputId": "0ad54388-02b8-4888-a792-2c1d1a668d36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 1 1], shape=(3,), dtype=int64)\n",
      "tf.Tensor([0 2 2], shape=(3,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Random tensor\n",
    "a = tf.constant([[5,2,3],[26,56,92],[3,0,26]])\n",
    "\n",
    "# Axis=0 tells you the maximum value location in each column (reduces along the rows)\n",
    "a1 = tf.argmax(a,axis=0)\n",
    "# Axis=1 tells you the maximum value location in each row (reduces along the columns)\n",
    "a2 = tf.argmax(a,axis=1)\n",
    "\n",
    "# Output argmax tensors to output window\n",
    "print(a1)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1kEraeQ1zG-6"
   },
   "source": [
    "## XOR Gate Neural Network\n",
    "Input data and correct output:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1dgqtiFQPmhIpVdrX498kiIVAgAz-e3T-\" width=\"300\" height=\"150\" />\n",
    "\n",
    "Neural Network Model:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1Q57rgI33Krm5hememSpJPNcDKGKz-ZT1\" width=\"500\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q408LlPToEr9"
   },
   "outputs": [],
   "source": [
    "# Calculate output based on input from XOR gate\n",
    "def output_hidden_layer1(input):\n",
    "    # Given weights\n",
    "    w_1= tf.constant([[-4, -6, -5],[3, 6, 4]], dtype = 'float32')\n",
    "    b_1 = tf.constant([-2, 3, -2], dtype = 'float32')\n",
    "  \n",
    "    # Multiply input of 1x2 tensor by 2x3 tensor and add 1x3 bias\n",
    "    output_H1 = tf.matmul(input, w_1) + b_1\n",
    "    output_H1_activation = tf.sigmoid(output_H1)\n",
    "\n",
    "    # Hidden layer two\n",
    "    # w2 Must be a column vector\n",
    "    w_2 = tf.constant([[5],[-9],[7]], dtype = 'float32')\n",
    "    b_output = tf.constant([[4]], dtype = 'float32') # single bias from 1 node output\n",
    "    output_layer_pre_activation = tf.matmul(output_H1_activation, w_2) + b_output\n",
    "    output_layer_activation = tf.sigmoid(output_layer_pre_activation)\n",
    "  \n",
    "    return output_layer_activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "BkqOMOqp7fOX",
    "outputId": "5ecf3273-2de9-4dc1-d570-c0a11b5a3bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Input 1 |   Input 2 |   True Output |   Computed Output |       Error |\n",
      "|-----------+-----------+---------------+-------------------+-------------|\n",
      "|         0 |         0 |             0 |         0.0413786 |  0.0413786  |\n",
      "|         1 |         0 |             1 |         0.973193  | -0.0268073  |\n",
      "|         0 |         1 |             1 |         0.992014  | -0.00798649 |\n",
      "|         1 |         1 |             0 |         0.0179147 |  0.0179147  |\n"
     ]
    }
   ],
   "source": [
    "# Surround with double brackets to make the sizes match with weights \n",
    "# All inputs for XOR gate\n",
    "input1 = tf.constant([[0,0]], dtype = 'float32')\n",
    "input2 = tf.constant([[1,0]], dtype = 'float32')\n",
    "input3 = tf.constant([[0,1]], dtype = 'float32')\n",
    "input4 = tf.constant([[1,1]], dtype = 'float32')\n",
    "\n",
    "comp_output_1 = output_hidden_layer1(input1)\n",
    "comp_output_2 = output_hidden_layer1(input2)\n",
    "comp_output_3 = output_hidden_layer1(input3)\n",
    "comp_output_4 = output_hidden_layer1(input4)\n",
    "\n",
    "# 𝐸𝑟𝑟𝑜𝑟 = (𝐶𝑜𝑚𝑝𝑢𝑡𝑒𝑑𝑂𝑢𝑡𝑝𝑢𝑡 − 𝑇𝑟𝑢𝑒𝑂𝑢𝑡𝑝𝑢𝑡)^2\n",
    "error1 = (comp_output_1 - tf.constant([[0]], dtype = 'float32'))\n",
    "error2 = (comp_output_2 - tf.constant([[1]], dtype = 'float32'))\n",
    "error3 = (comp_output_3 - tf.constant([[1]], dtype = 'float32'))\n",
    "error4 = (comp_output_4 - tf.constant([[0]], dtype = 'float32'))\n",
    "\n",
    "#Output to terminal\n",
    "l = [[0, 0, 0, comp_output_1, error1], [1, 0, 1, comp_output_2, error2],\n",
    "     [0, 1, 1, comp_output_3, error3], [1,1,0, comp_output_4, error4]]\n",
    "table = tabulate(l, headers=['Input 1', 'Input 2', 'True Output', 'Computed Output', 'Error'], tablefmt='orgtbl')\n",
    "\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "activationFunctions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
